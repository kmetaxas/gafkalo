---
topics:
  - name: SKATA.VROMIA.POLY
    partitions: 6
    replication_factor: 1
    configs:
      cleanup.policy: delete,compact
      min.insync.replicas: 1
      retention.ms: 123456789

    key:
      schema: "schemas/schema-key.json"
      compatibility: BACKWARD
    value:
      # Schema points to the filename of the schema.
      # it can be absolute or relative to gafaklo binary.
      # if schema_dir is in kafkalo config then it is prepended to schemas
      schema: "schemas/schema.json"
      compatibility: NONE
  - name: SKATA.VROMIA.LIGO
    partitions: 6
    replication_factor: 3
    configs:
      cleanup.policy: delete
      min.insync.replicas: 1
    key:
      schema: "schemas/schema-key.json"
      schema_type: "AVRO"
  - name: SKATA1
    partitions: 12
    replication_factor: 1
    configs:
      cleanup.policy: delete
      retention.ms: 123456789
  - name: SKATA2
    partitions: 1
    replication_factor: 1
  - name: SKATA3
    partitions: 1
    replication_factor: 1
  - name: SKATA4
    partitions: 1
    replication_factor: 1
  - name: SKATA5
    partitions: 1
    replication_factor: 1
  - name: SKATA6
    partitions: 1
    replication_factor: 1
  - name: SKATA7
    partitions: 1
    replication_factor: 1

clients:
  - principal: User:poutanaola
    consumer_for:
      - topic: TOPIC1.
      - topic: TOPIC2.
        isLiteral: true
      - topic: TOPICLOL
    producer_for:
      - topic: TOPIC1.
    resourceowner_for:
      - topic: TOPIC4.
    transactional_ids:
      - name: MYID
  - principal: Group:malakes
    consumer_for:
      - topic: TOPIC1.
      - topic: TOPIC2.
    producer_for:
      - topic: TOPIC1.
        isLiteral: true
  - principal: User:produser
    producer_for:
      - topic: TOPIC1.
        # Strict mode is mean for production.
        # It will make the producer able to write the topics but read-only
        # access to the schema registry
        strict: true
        idempotent: true
    groups:
      - name: consumer-produser-
      - name: consumer-produser-owner-
        # if not specified, roles is [DeveloperRead]
        roles: ["ResourceOwner"]
        isLiteral: true
    # IT is also possible to set rolebindings directly, instead of the "convenience" settings . 
    # the format is as follows:
    rolebindings:
      # context can be one of:
      # KAFKA | SR | KSQLDB | CONNECT 
      - context: KAFKA 
        # Scope can be one of CLUSTER | RESOURCE
        scope: RESOURCE
        # Role , for CLUSTER or RESOURCE scopes can be one of
        # DeveloperRead | DeveloperWrite | ResourceOwner | DeveloperManage
        # Or, for CLUSTER scope onle, one of:
        # SystemAdmin | ClusterAdmin | UserAdmin | SecurityAdmin | AuditAdmin | Operator
        role: DeveloperRead
        # Resource is only needed when scope=RESOURCE. It should be empty otherwise
        # So examples for resource type are:
        # Topic | Subject | Group | Connector | TransactionalId
        resources:
          - name: trololo
            resource_type: Topic
            # Pattern type can be one of:
            # LITERAL | PREFIXED
            pattern_type: LITERAL
          - name: Skata
            resource_type: Topic
            pattern_type: PREFIXED
      - context: SR
        scope: CLUSTER
        role: SecurityAdmin 
        # Pattern type 
        pattern_type: LITERAL
connectors:
  - name: skata1
    config:
      connector.class: "io.confluent.connect.s3.S3SinkConnector"
      transforms: "Archive"
      errors.log.enable: "true"
      errors.log.include.messages: "true"
      topics: "SKATA1"
      transforms.Archive.type: "com.github.jcustenborder.kafka.connect.archive.Archive"
      store.url: "http://127.0.0.1:9000"
      s3.bucket.name: "testsmt"
      s3.part.size: "5242880"
      flush.size: "1000"
      storage.class: "io.confluent.connect.s3.storage.S3Storage"
      format.class: "io.confluent.connect.s3.format.avro.AvroFormat"
      schema.generator.class: "io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator"
      aws.secret.access.key: "minioadmin"
      aws.access.key.id: "minioadmin"
      partitioner.class: "io.confluent.connect.storage.partitioner.DefaultPartitioner"
      schema.compatibility: "NONE"
