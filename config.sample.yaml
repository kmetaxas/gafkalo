# Configs to reach out all API endpoints
connections:
  kafka:
    # look at librdkafka docs
    bootstrapBrokers: ["localhost:9093"]
    ssl:
      enabled: false
      caPath: "/path/to/ca.crt"
    # Kerberos (sasl GSSAPI) authentication
    kerberos:
      enabled: true
      keytab: "path_to.key" # Will disable user auth if specified
      serviceName: "kafka"
      realm: "EXAMPLE.COM"
      username: "userame"
      password: "password"
    # Override producer configs
    producer:
      compression: "snappy"
      maxMessageBytes: 4000000
    # SASL_PLAINTEXT (SCRAM) authentication support
    scram:
      enabled: false
      username: "username"
      password: "password"
    tokenauth:
      enabled: true
      client: trololo
      secret: there_is_no_password_only_zuul
      url: http://localhost:8090/security/1.0/authenticate
      # If set to true, this tells Gafkalo to use a token provider that fetches token from a Confluent Metadata service api
      is_confluent_mds: true
      caPath: /path/to/ca.pem
  schemaregistry:
    url: "http://localhost:8081"
    username: "user"
    password: "password!"
    caPath: "/home/arcanum/Downloads/ca.crt" # Add a CA to trust store
    skipVerify: true # Skip TLS verify
  mds:
    url: "http://localhost:8090"
    username: "username"
    password: "password"
    schema-registry-cluster-id: "schemaregistry"
    caPath: "/home/arcanum/Downloads/ca.crt" # Add a CA to trust store
    skipVerify: false # Skip TLS verify

# App specific configs
kafkalo:
  input_dirs:
    - "data/*"
    - "data/team2.yaml"
  # If schema_dir is specified, gafkalo will prepend this to the schema path in each schema definition. This makes it easy to have schema files relative to a shared path.
  schema_dir: "data/"
  # regex pattern that , if matched , will hide from plan output as sensitve
  connectors_sensitive_keys: "^.*(auth|password|credential).*$"
